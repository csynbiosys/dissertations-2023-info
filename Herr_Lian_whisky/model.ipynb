{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3834320e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f6/7tdb25f1467_4vlhypfm_q380000gn/T/ipykernel_23220/2538930681.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Convert date data to timestamps\n",
    "    date_columns = ['Auction Date']\n",
    "    for col in date_columns:\n",
    "        data[col] = pd.to_datetime(data[col]).astype('int64') / 10 ** 9\n",
    "\n",
    "    # Use LabelEncoder to process categorical data\n",
    "    categorical_columns = ['Distillery', 'Brand', 'Type', 'Packing', 'Country']\n",
    "    for col in categorical_columns:\n",
    "        le = label_encoders.get(col)\n",
    "        if le:\n",
    "            data[col] = le.transform(data[col].astype(str))\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    return data\n",
    "\n",
    "label_encoders = {}\n",
    "train_data = preprocess_data(train_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "\n",
    "# Separate features and target variables\n",
    "X_train = train_data.drop('Result', axis=1)\n",
    "y_train = train_data['Result']\n",
    "\n",
    "X_test = test_data.drop('Result', axis=1)\n",
    "y_test = test_data['Result']\n",
    "\n",
    "# Define XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 0.9, 1],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_lambda': [0.5, 1, 1.5],\n",
    "    'reg_alpha': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Use Repeated K-Fold Cross Validation for hyperparameter tuning\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=rkf, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the model with best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Visual analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Diagonal line\n",
    "plt.title('Actual Value vs Predicted Value')\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.show()\n",
    "\n",
    "# 1. Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.show()\n",
    "\n",
    "# 2. Histogram of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='k')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature importance plot\n",
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(features, feature_importances, align='center')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Significance')\n",
    "plt.ylabel('Diagnostic Property')\n",
    "plt.show()\n",
    "\n",
    "print(feature_importances)\n",
    "\n",
    "# 1. Box plots of predicted versus actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([y_test, y_pred], vert=False, labels=['Predicted Value', 'Actual Value'])\n",
    "plt.title('Box Plots of Predicted versus Actual Values')\n",
    "plt.xlabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# 2. Learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(XGBRegressor(), X_train, y_train, train_sizes=[0.1, 0.33, 0.55, 0.78, 1.], cv=5)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(train_sizes, valid_scores.mean(axis=1), 'o-', color=\"g\", label=\"Cross-validation Score\")\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Validation curve (example using max_depth of XGBoost)\n",
    "param_range = np.arange(1, 10, 1)\n",
    "train_scores, valid_scores = validation_curve(XGBRegressor(), X_train, y_train, param_name=\"max_depth\", param_range=param_range, cv=5)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_range, train_scores.mean(axis=1), 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(param_range, valid_scores.mean(axis=1), 'o-', color=\"g\", label=\"Cross-validation Score\")\n",
    "plt.title('Validation Curve - max_depth')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Initialize JavaScript visualization code for SHAP\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(model, feature_perturbation='interventional')\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_train, check_additivity=False)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "for feature in X_train.columns:\n",
    "    shap.dependence_plot(feature, shap_values, X_train, display_features=X_train)\n",
    "# For the first instance in the dataset\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5562d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
