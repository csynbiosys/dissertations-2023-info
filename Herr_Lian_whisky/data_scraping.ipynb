{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import threading\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variable to control which website to crawl\n",
    "selected_website = \"whiskyauction\"  # Change this to \"website2\" to select the second website\n",
    "selected_type = 'test'              #Type test for testing, or other for global crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def crawl_website1():\n",
    "    # Base URL and header information\n",
    "    url = \"https://whiskyauction.com/wac/whiskyBrowserData\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Getting the total number of records\n",
    "    params = {\n",
    "        \"start\": 0,\n",
    "        \"length\": 9,\n",
    "        \"currentsort\": \"auction_prose\",\n",
    "        \"currentsortdirection\": \"DESC\",\n",
    "        \"currentview\": \"card\",\n",
    "        \"filterToBeApplied\": 0\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    data_tmp = response.json()\n",
    "    total = data_tmp['recordsTotal']\n",
    "\n",
    "    # Calculate the total number of pages\n",
    "    items_per_page = 100\n",
    "    total_pages = total // items_per_page + (1 if total % items_per_page else 0)\n",
    "\n",
    "    all_data = []\n",
    "    fieldnames = [\"Id\", \"Distillery\", \"Barrel\", 'Brand', \"Alcohol Strength\", \"Quantity\", \"Auction Date\",\n",
    "                  \"Distilled Date\",\n",
    "                  \"Bottled Date\", \"Age\", \"Type\", \"Packing\", \"Series\", \"Country\", \"Result\"]\n",
    "\n",
    "    def process_pages(start_page, end_page, lock):\n",
    "        if selected_type =='test':\n",
    "            end_page = 1\n",
    "        for page in range(start_page, end_page):\n",
    "            start = page * items_per_page\n",
    "            data = {\n",
    "                \"start\": start,\n",
    "                \"length\": items_per_page,\n",
    "                \"currentsort\": \"auction_prose\",\n",
    "                \"currentsortdirection\": \"DESC\",\n",
    "                \"currentview\": \"card\",\n",
    "                \"filterToBeApplied\": 0\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, headers=headers, data=data)\n",
    "            data = response.json()\n",
    "            page_data = data['data']\n",
    "\n",
    "            # Parsing the data on each page\n",
    "            page_results = []\n",
    "            for item in page_data:\n",
    "                whisky_info = {\n",
    "                    \"Id\": item[\"item_id\"],\n",
    "                    \"Distillery\": item[\"distillery_prose\"],\n",
    "                    \"Barrel\": item[\"barrel\"],\n",
    "                    \"Brand\": item[\"bottler_label_prose\"],\n",
    "                    \"Alcohol Strength\": item[\"alcohol_strength\"],\n",
    "                    \"Quantity\": item[\"content_quantity\"],\n",
    "                    \"Auction Date\": item[\"auction_prose\"],\n",
    "                    \"Distilled Date\": item[\"distilled_date\"],\n",
    "                    \"Bottled Date\": item[\"bottled_date\"],\n",
    "                    \"Age\": item[\"age\"],\n",
    "                    \"Type\": item[\"whisky_type_prose\"],\n",
    "                    \"Packing\": item[\"packing_type_prose\"],\n",
    "                    \"Series\": item[\"series_prose\"],\n",
    "                    \"Country\": item[\"country_prose\"],\n",
    "                    \"Result\": item[\"result\"]\n",
    "                }\n",
    "                page_results.append(whisky_info)\n",
    "\n",
    "            # Use locks to ensure that only one thread writes to a file at a time\n",
    "            with lock:\n",
    "                with open('whisky_data.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    for info in page_results:\n",
    "                        writer.writerow(info)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Creating a Lock Object\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    # Create and start multiple threads\n",
    "    num_threads = 10\n",
    "    pages_per_thread = total_pages // num_threads\n",
    "\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        start_page = i * pages_per_thread\n",
    "        end_page = start_page + pages_per_thread if i != num_threads - 1 else total_pages\n",
    "        thread = threading.Thread(target=process_pages, args=(start_page, end_page, lock))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    for info in all_data:\n",
    "        print(info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crawl_website2():\n",
    "    # List of stored URLs\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(1, 92):\n",
    "        surl = f\"https://www.whiskyhammer.com/auction/past/auc-{i}\"\n",
    "        url_list.append(surl)\n",
    "        print(url_list)\n",
    "    html_list = url_list\n",
    "    if selected_type == 'test':\n",
    "        html_list = 'https://www.whiskyhammer.com/auction/past/auc-92'\n",
    "\n",
    "    pagecount = 0\n",
    "    data = []\n",
    "    data_page = []\n",
    "    count = 0\n",
    "    count_attr = {'Distillery': 0, 'Age': 0, 'Country': 0, 'Bottles Produced': 0,\n",
    "                  'Region': 0, 'Staus': 0, 'Whisky Type': 0, 'Size': 0,\n",
    "                  'Strength': 0}\n",
    "    with open('processed_data.txt', 'w') as f:\n",
    "        pass\n",
    "\n",
    "    def process_lot(lot_url):\n",
    "        try:\n",
    "            # Go to the lot page\n",
    "\n",
    "            response = requests.get(lot_url)\n",
    "            response.raise_for_status()  # Raise an exception if the response status is not 200\n",
    "\n",
    "            distillery = lot_url.split('/')[5]\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the required information\n",
    "            result_head = soup.find('a', {\"class\": 'button alt'})\n",
    "            Id = int(str(result_head).strip('<a class=\"button alt\" href=\"/contact?_id=>Ask a question</a>'))\n",
    "\n",
    "            result = soup.find_all('div', {'class': 'inner'})\n",
    "            attr_list = []\n",
    "            for tag in result:\n",
    "                attr_list.append(tag.text.strip())\n",
    "\n",
    "            temp = {'Id': Id, 'Distillery': distillery,\n",
    "                    'Bottles Produced': 'not show',\n",
    "                    'Region': 'not show', 'Staus': 'not show', 'Whisky Type': 'not show', 'Size': 'not show',\n",
    "                    'Strength': 'not show'}\n",
    "\n",
    "            attr_name = [\"Bottles Produced\", \"Type\", \"Size\", \"Region\",\n",
    "                         'Strength']\n",
    "            for attr in attr_list:\n",
    "                for name in attr_name:\n",
    "\n",
    "                    if \"Distillery Status\" in attr:\n",
    "                        temp['Staus'] = attr\n",
    "                        count_attr['Staus'] = count_attr['Staus'] + 1\n",
    "                        break\n",
    "                    if \"Type\" in attr:\n",
    "                        temp['Whisky Type'] = attr\n",
    "                        count_attr['Whisky Type'] = count_attr['Whisky Type'] + 1\n",
    "                        break\n",
    "                    if name in attr:\n",
    "                        temp[name] = attr\n",
    "                        count_attr[name] = count_attr[name] + 1\n",
    "                        break\n",
    "\n",
    "            distillery = temp[\"Distillery\"]\n",
    "            NumBottle = temp['Bottles Produced'].replace(\"Bottles Produced\\n                                        \",\n",
    "                                                         \"\")\n",
    "            Type = temp['Whisky Type'].replace(\"Type\\n                                        \", \"\")\n",
    "            Amount = temp['Size'].replace(\"Size\\n                                        \", \"\")\n",
    "            region = temp['Region'].replace(\"Region\\n                                        \", \"\")\n",
    "            # country = temp['Country'].replace(\"Country\", \"\")\n",
    "            Staus = temp['Staus'].replace(\"Distillery Status\\n                                        \", \"\")\n",
    "            Strength = temp['Strength'].replace(\"Strength\\n                                        \", \"\")\n",
    "\n",
    "            # Add the data for this lot to the list\n",
    "            data.append([Id, distillery, Staus, NumBottle, Type, Amount, Strength, region])\n",
    "            for row in data:\n",
    "                if len(row) != 8:  # 根据你的代码，我们假设数据应该有8列\n",
    "                    print(row)\n",
    "            print([Id, distillery, Staus, NumBottle, Type, Amount, Strength, region])\n",
    "            with open('processed_data.txt', 'a') as f:\n",
    "                f.write(f\"{Id},{distillery},{Staus},{NumBottle},{Type},{Amount},{Strength},{region}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while fetching {lot_url}: {e}\")\n",
    "\n",
    "    def extract_age_from_wine_name(wine_name):\n",
    "        pattern = r'\\b(\\d+)\\s*YEAR\\s*OLD\\b'\n",
    "        match = re.search(pattern, wine_name, re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            age = int(match.group(1))\n",
    "            return age\n",
    "        else:\n",
    "            return 'not shown'\n",
    "\n",
    "    for htmls in html_list:\n",
    "        with open('processed_data.txt', 'w') as f:\n",
    "            pass\n",
    "        pagecount = pagecount + 1\n",
    "        print(pagecount)\n",
    "        print(\"------------new month -------------\")\n",
    "        response_html = requests.get(htmls)\n",
    "        html_soup = BeautifulSoup(response_html.text, 'html.parser')\n",
    "        numLots = int(html_soup.find('div', {'class': 'numberOfProducts'}).text.strip(\" \\n\").strip('Items'))\n",
    "        pagenum = numLots // 50 + 1\n",
    "        print(pagenum)\n",
    "        lot_urls = []\n",
    "        for page in range(1, pagenum + 1):\n",
    "            page_url = htmls + \"?page=\" + str(page)\n",
    "            print(\"start page\" + str(page))\n",
    "            response_page = requests.get(page_url)\n",
    "            print('page' + str(page) + 'response')\n",
    "            page_soup = BeautifulSoup(response_page.text, 'html.parser')\n",
    "            page_text = str(page_soup)\n",
    "            pattern = r':items=\"([^\"]+)\"'\n",
    "            match = re.search(pattern, page_text)\n",
    "            if match and match.group(1):\n",
    "                items_attribute = match.group(1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            decoded_items_attribute = html.unescape(items_attribute)\n",
    "            auction_list = json.loads(decoded_items_attribute)\n",
    "\n",
    "            for item in auction_list:\n",
    "                url = item.get(\"url\")\n",
    "                lotid = item.get(\"id\")\n",
    "                name = item.get(\"name\")\n",
    "                highest_bid = item.get(\"item_price\")\n",
    "                time = item.get(\"ends\")\n",
    "                age = extract_age_from_wine_name(name)\n",
    "                count_page_attr = [lotid, name, age, highest_bid, time]\n",
    "                data_page.append([lotid, name, age, highest_bid, time])\n",
    "                print(count_page_attr)\n",
    "                if url:\n",
    "                    lot_urls.append(url)\n",
    "        lot_urls = list(set(lot_urls))\n",
    "\n",
    "        batch_size = 50  # Setting the size of each batch\n",
    "\n",
    "        for batch_start in range(0, len(lot_urls), batch_size):\n",
    "            batch_urls = lot_urls[batch_start:batch_start + batch_size]\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = [executor.submit(process_lot, lot_url) for lot_url in batch_urls]\n",
    "                concurrent.futures.wait(futures)\n",
    "        data = []\n",
    "\n",
    "        # Read the processed data from the file and merge it.\n",
    "\n",
    "        with open('processed_data.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) == 8:\n",
    "                    data.append(values)\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['Id', 'distillery', 'Staus', 'Bottles Produced', 'type',\n",
    "                                         'amount',\n",
    "                                         'Strength',\n",
    "                                         'region'])\n",
    "\n",
    "        df_page = pd.DataFrame(data_page, columns=['Id', 'name', 'vintage', 'highest_bid', 'time'])\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "    df.to_csv('./whiskyhammer/auction_data'  + '.csv', index=False)\n",
    "    df_page.to_csv('./whiskyhammer/auction_data_page_all' + '.csv', index=False)\n",
    "    print(count)\n",
    "    print(count_attr)\n",
    "\n",
    "def data_transfer():\n",
    "    new_columns = [\"Id\", \"Barrel\", \"Distillery\", \"Brand\", \"Alcohol Strength\", \"Quantity\", \"Auction Date\",\n",
    "                   \"Distilled Date\", \"Bottled Date\", \"Age\", \"Type\", \"Packing\", \"Series\", \"Country\", \"Result\"]\n",
    "    existing_data_path = 'whisky_data.csv'\n",
    "    existing_df = pd.read_csv(existing_data_path, names=new_columns, header=None, skiprows=1)\n",
    "\n",
    "    # Save the modified DataFrame back to the same file\n",
    "    existing_df.to_csv(existing_data_path, index=False)\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    def detect_date(text):\n",
    "        # 使用Spacy解析文本\n",
    "        doc = nlp(text)\n",
    "        # 提取日期实体\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'DATE':\n",
    "                try:\n",
    "                    # 尝试将识别到的日期转换为所需格式\n",
    "                    return pd.to_datetime(ent.text).strftime('%Y-%m')\n",
    "                except:\n",
    "                    pass\n",
    "        return None\n",
    "\n",
    "    # 读取数据\n",
    "    df = pd.read_csv('whisky_data.csv')\n",
    "\n",
    "    date_columns = ['Auction Date', 'Distilled Date', 'Bottled Date']\n",
    "    for col in date_columns:\n",
    "        df[col] = df[col].apply(lambda x: detect_date(str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "    # 保存更新后的数据\n",
    "    df.to_csv('whisky_data.csv', index=False)\n",
    "\n",
    "    # Fill missing Auction Date with '2020-12'\n",
    "    df['Auction Date'].fillna('2020-12', inplace=True)\n",
    "\n",
    "    # Function to process Age\n",
    "    def process_age(row):\n",
    "        if pd.isna(row['Age']) or row['Age'] == 0:\n",
    "            if pd.notna(row['Bottled Date']) and pd.notna(row['Distilled Date']):\n",
    "                year_bottled = int(row['Bottled Date'].split('-')[0])\n",
    "                year_distilled = int(row['Distilled Date'].split('-')[0])\n",
    "                return year_bottled - year_distilled\n",
    "        return row['Age']\n",
    "\n",
    "    # Function to process Bottled Date and Distilled Date\n",
    "    def process_dates(row):\n",
    "        if pd.isna(row['Bottled Date']) and pd.isna(row['Distilled Date']) and (\n",
    "                pd.notna(row['Age']) and row['Age'] != 0):\n",
    "            if isinstance(row['Auction Date'], str) and '-' in row['Auction Date']:\n",
    "                year_auction = int(row['Auction Date'].split('-')[0])\n",
    "                year_calculated = year_auction - int(row['Age'])\n",
    "                date_str = str(year_calculated) + '-' + row['Auction Date'].split('-')[1]\n",
    "                row['Bottled Date'] = row['Distilled Date'] = date_str\n",
    "        return row\n",
    "\n",
    "    # Apply functions to process data\n",
    "    df['Age'] = df.apply(process_age, axis=1)\n",
    "    df = df.apply(process_dates, axis=1)\n",
    "\n",
    "    # Drop rows with missing Bottled Date, Distilled Date, and Age (or Age is 0)\n",
    "    df.drop(df[(df['Bottled Date'].isna() & df['Distilled Date'].isna() & (df['Age'].isna() | df['Age'] == 0))].index,\n",
    "            inplace=True)\n",
    "    # Drop rows with missing Bottled Date, Distilled Date, and Age\n",
    "    df.drop(df[(df['Bottled Date'].isna() & df['Distilled Date'].isna() & df['Age'].isna())].index, inplace=True)\n",
    "\n",
    "    # Fill missing Packing values\n",
    "    df['Packing'].fillna('Bottle', inplace=True)\n",
    "\n",
    "    # Drop the 'Series' column\n",
    "    df.drop(columns=['Series'], inplace=True)\n",
    "\n",
    "    # Save processed data\n",
    "    df.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('processed_data.csv')\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Drop rows where Alcohol Strength or Quantity is missing\n",
    "    df.dropna(subset=['Alcohol Strength', 'Quantity'], inplace=True)\n",
    "\n",
    "    # Fill Distilled Date when Bottled Date and Age are present\n",
    "    def fill_distilled_date(row):\n",
    "        if pd.notna(row['Bottled Date']) and pd.notna(row['Age']) and pd.isna(row['Distilled Date']):\n",
    "            year_bottled = int(row['Bottled Date'].split('-')[0])\n",
    "            year_calculated = year_bottled - int(row['Age'])\n",
    "            return str(year_calculated) + '-' + row['Bottled Date'].split('-')[1]\n",
    "        return row['Distilled Date']\n",
    "\n",
    "    df['Distilled Date'] = df.apply(fill_distilled_date, axis=1)\n",
    "\n",
    "    # Fill Bottled Date when Distilled Date and Age are present\n",
    "    def fill_bottled_date(row):\n",
    "        if pd.notna(row['Distilled Date']) and pd.notna(row['Age']) and pd.isna(row['Bottled Date']):\n",
    "            year_distilled = int(row['Distilled Date'].split('-')[0])\n",
    "            year_calculated = year_distilled + int(row['Age'])\n",
    "            return str(year_calculated) + '-' + row['Distilled Date'].split('-')[1]\n",
    "        return row['Bottled Date']\n",
    "\n",
    "    df['Bottled Date'] = df.apply(fill_bottled_date, axis=1)\n",
    "\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_data = df.isna().sum()\n",
    "\n",
    "    # Output columns with missing values\n",
    "    print(\"Columns with missing values:\")\n",
    "    for column, missing_count in missing_data.items():\n",
    "        if missing_count > 0:\n",
    "            print(f\"{column}: {missing_count} missing values\")\n",
    "\n",
    "    # Load CSV file\n",
    "    csv_file_path = 'processed_data.csv'  # Replace with your CSV data file path\n",
    "    data_frame = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Drop the 'Distilled Date' and 'Bottled Date' columns\n",
    "    data_frame = data_frame.drop(columns=['Distilled Date'])\n",
    "    data_frame = data_frame.drop(columns=['Bottled Date'])\n",
    "\n",
    "    # Save the processed data as a CSV file\n",
    "    output_csv_file_path = 'processed_data_new.csv'  # Replace with the desired path to save the CSV file\n",
    "    data_frame.to_csv(output_csv_file_path, index=False)\n",
    "    # Load existing data, skipping the first row\n",
    "    existing_df = pd.read_csv('processed_data.csv', skiprows=1)\n",
    "\n",
    "    # Save the modified DataFrame back to the same file\n",
    "    existing_df.to_csv('processed_data.csv', index=False)\n",
    "    print(\"Processing completed!\")\n",
    "\n",
    "\n",
    "if selected_website == \"whiskyauction\":\n",
    "    crawl_website1()\n",
    "    data_transfer()\n",
    "elif selected_website == \"whiskyhammer\":\n",
    "    crawl_website2()\n",
    "else:\n",
    "    print(\"Invalid selection. Please choose 'website1' or 'website2'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
